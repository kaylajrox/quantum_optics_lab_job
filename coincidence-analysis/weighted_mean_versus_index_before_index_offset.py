import csv
import re
import numpy as np
from pathlib import Path
from datetime import datetime

script_name = Path(__file__).name  # Track which script generated this
time_per_sample = 1.0  # Adjust to your actual time resolution
data_directory = "20250507_more_peaks_compare_coicdence"

# === Paths ===
script_dir = Path(__file__).resolve().parent
data_dir = script_dir.parent / "data-photon-counts-SiPM" / data_directory
weighted_csv_path = script_dir / "processed_peak_data.csv"

# === Load Existing CSV ===
weighted_rows = []
with open(weighted_csv_path, 'r') as wf:
    reader = csv.DictReader(wf)
    for row in reader:
        weighted_rows.append(row)

# === Compute Weighted Means for Each Peak ===
print(f"Searching for subdirectories in: {data_dir}")
peak_dirs = [subdir for subdir in data_dir.rglob("*") if subdir.is_dir() and subdir.name.startswith("peak")]

for peak_dir in peak_dirs:
    folder_name = peak_dir.name
    parts = folder_name.split("_")

    first_peak = parts[0].replace("peak", "")
    second_peak = parts[1].replace("and", "")
    coincidence = f"Peak {first_peak} and {second_peak}"
    correlation_time = parts[2]
    state = next((word for word in ["filtered", "unfiltered", "raw"] if word in parts), "")

    data_files = list(peak_dir.glob("*.txt"))

    for file_path in data_files:
        if "AddBack" not in file_path.name:
            continue

        # === Load Data ===
        data = np.loadtxt(file_path)
        indices = np.arange(len(data))

        if data.sum() == 0:
            print(f"[WARNING] Data in {file_path.name} is all zeros. Skipping.")
            continue

        # === Calculate Weighted Mean ===
        weighted_mean_index = np.average(indices, weights=data)
        weighted_mean_time = weighted_mean_index * time_per_sample

        print(
            f"File: {file_path.name} | Weighted Mean Index: {weighted_mean_index:.2f} | Time: {weighted_mean_time:.2f}")

        # === Update matching row in CSV ===
        for row in weighted_rows:
            if (row["File"] == file_path.name and
                    row["Coincidence"] == coincidence and
                    row["Correlation Time"] == correlation_time and
                    row["State"] == state):
                row["Weighted Mean Index (calculated)"] = f"{weighted_mean_index:.2f}"
                row["Weighted Mean Time (calculated)"] = f"{weighted_mean_time:.2f}"
                row["Generated By"] = script_name
                row["Time Ran"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# === Write Updated CSV ===
fieldnames = list(weighted_rows[0].keys())
with open(weighted_csv_path, 'w', newline='') as wf:
    writer = csv.DictWriter(wf, fieldnames=fieldnames)
    writer.writeheader()
    writer.writerows(weighted_rows)

print(f"âœ… Updated {weighted_csv_path} with calculated weighted means and provenance info.")

# # rate it moves out seems reasonable constant
#
# # todo make the table
#
# import pandas as pd
# import matplotlib.pyplot as plt
# import re
#
# # === CONFIG ===
# csv_path = "processed_peak_data.csv"  # Adjust if needed
# font_size = 16
#
# # === LOAD AND PARSE CSV ===
# df = pd.read_csv(csv_path)
#
# # Extract second peak number from the Coincidence column
# def extract_second_peak(coincidence_str):
#     match = re.search(r"Peak \d+ and (\d+)", str(coincidence_str))
#     return int(match.group(1)) if match else None
#
# df["Second Peak Number"] = df["Coincidence"].apply(extract_second_peak)
# df = df.dropna(subset=["Second Peak Number", "Weighted Mean Index", "State"])
#
# # Separate by state
# df_filtered = df[df["State"] == "filtered"].sort_values("Second Peak Number")
# df_unfiltered = df[df["State"] == "unfiltered"].sort_values("Second Peak Number")
#
# # === FILTERED PLOT ===
# plt.figure(figsize=(10, 6))
# plt.plot(df_filtered["Second Peak Number"], df_filtered["Weighted Mean Index"], 'o-', color='blue', label='Filtered', linewidth=2)
# plt.xlabel("Peak Number", fontsize=font_size)
# plt.ylabel("Weighted Mean Index", fontsize=font_size)
# plt.title("Filtered: Weighted Mean Index vs. Second Peak Number", fontsize=font_size)
# plt.grid(True)
# plt.xticks(fontsize=font_size)
# plt.yticks(fontsize=font_size)
# plt.tight_layout()
# plt.show()
#
# # === UNFILTERED PLOT ===
# plt.figure(figsize=(10, 6))
# plt.plot(df_unfiltered["Second Peak Number"], df_unfiltered["Weighted Mean Index"], 's--', color='orange', label='Unfiltered', linewidth=2)
# plt.xlabel("Peak Number", fontsize=font_size)
# plt.ylabel("Weighted Mean Index", fontsize=font_size)
# plt.title("Unfiltered: Weighted Mean Index vs. Second Peak Number", fontsize=font_size)
# plt.grid(True)
# plt.xticks(fontsize=font_size)
# plt.yticks(fontsize=font_size)
# plt.tight_layout()
# plt.show()
#
#
#
#
# # import pandas as pd
# # import matplotlib.pyplot as plt
# # import re
# #
# # # === CONFIG ===
# # csv_path = "weighted_means2.csv"  # Adjust if needed
# # font_size = 16
# #
# # # === LOAD AND PARSE CSV ===
# # df = pd.read_csv(csv_path)
# #
# # # Extract second peak number
# # def extract_second_peak(coincidence_str):
# #     match = re.search(r"Peak \d+ and (\d+)", str(coincidence_str))
# #     return int(match.group(1)) if match else None
# #
# # df["Second Peak Number"] = df["Coincidence"].apply(extract_second_peak)
# # df = df.dropna(subset=["Second Peak Number", "Weighted Mean Index"])
# #
# # # === SORT DATA (optional) ===
# # df = df.sort_values(by="Second Peak Number")
# #
# # # === PLOT ===
# # plt.figure(figsize=(10, 6))
# # plt.plot(df["Second Peak Number"], df["Weighted Mean Index"], 'o-', linewidth=2)
# # plt.xlabel("Peak Number", fontsize=font_size)
# # plt.ylabel("Weighted Mean Index", fontsize=font_size)
# # plt.title("Weighted Mean Index vs. Second Peak Number", fontsize=font_size)
# # plt.grid(True)
# # plt.xticks(fontsize=font_size)
# # plt.yticks(fontsize=font_size)
# # plt.tight_layout()
# # plt.show()
